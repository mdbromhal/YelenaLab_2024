{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "5 Oct. 2024\n",
        "\n",
        "Using image labels to train object detection model on lego friends\n",
        "\n",
        "References:\n",
        "- https://cocodataset.org/#home (Coco model)\n",
        "- https://www.robots.ox.ac.uk/~vgg/software/via/via.html (Labeling online software)\n",
        "- https://www.kaggle.com/code/hmendonca/airbus-mask-rcnn-and-coco-transfer-learning (Coco transfer learning example)\n",
        "- https://github.com/ultralytics/yolov5/issues/980 (Coco additional classes)\n",
        "- ChatGPT (questions throughout the process)"
      ],
      "metadata": {
        "id": "nVVCIzhLbIqb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Parse Json file for data we'll use"
      ],
      "metadata": {
        "id": "qAjbhoGEbbtm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "S8cg5n0UbDPY"
      },
      "outputs": [],
      "source": [
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading Json file\n",
        "with open('/content/drive/MyDrive/Colab Notebooks/via_project_24Sep2024_18h40m_json.json') as f:\n",
        "  data = json.load(f)\n",
        "\n",
        "# data.keys()"
      ],
      "metadata": {
        "id": "daSR1xs2baD3"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example format for 1 image:\n",
        "\n",
        "\"lego-00.11.20-09.18.2024.jpg69105\":{\"filename\":\"lego-00.11.20-09.18.2024.jpg\",\"size\":69105,\"regions\":[{\"shape_attributes\":{\"name\":\"rect\",\"x\":215,\"y\":200,\"width\":98,\"height\":122},\"region_attributes\":{\"Name\":\"Lego Friend\"}}],\"file_attributes\":{}}"
      ],
      "metadata": {
        "id": "E2FpKnHAdlHv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "images = []\n",
        "annotations = []\n",
        "\n",
        "# Parsing through json file and identifying data stored for each image\n",
        "for key, value in data.items():\n",
        "  filename = value['filename']\n",
        "\n",
        "  # There's only one region per image, but we'll use a for loop just to be sure\n",
        "  for region in value['regions']:\n",
        "    shape_attributes = region['shape_attributes']\n",
        "    shape = shape_attributes['name']\n",
        "    x = shape_attributes['x']\n",
        "    y = shape_attributes['y']\n",
        "    width = shape_attributes['width']\n",
        "    height = shape_attributes['height']\n",
        "\n",
        "    obj = region['region_attributes']['Name']\n",
        "\n",
        "    # Calculate the bounding box coordinates\n",
        "    xmin = x\n",
        "    ymin = y\n",
        "    xmax = x + width\n",
        "    ymax = y + height\n",
        "\n",
        "    images.append(filename)\n",
        "    annotations.append({'bbox': [xmin, ymin, xmax, ymax],\n",
        "                        'obj': obj})"
      ],
      "metadata": {
        "id": "Kc3OkG7qeJfS"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(images)\n",
        "print()\n",
        "print(annotations)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UjiUY118gEYM",
        "outputId": "6b473ed7-4b04-4f61-c7ee-6bd3456385c2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['lego-00.11.20-09.18.2024.jpg', 'lego-01.06.20-09.18.2024.jpg', 'lego-02.15.20-09.18.2024.jpg', 'lego-03.10.20-09.18.2024.jpg', 'lego-03.10.20-09.18.2024.jpg', 'lego-04.13.20-09.18.2024.jpg', 'lego-05.11.20-09.18.2024.jpg', 'lego-05.14.20-09.18.2024.jpg', 'lego-06.07.20-09.18.2024.jpg', 'lego-06.07.20-09.18.2024.jpg', 'lego-07.08.20-09.18.2024.jpg', 'lego-07.08.20-09.18.2024.jpg', 'lego-07.12.20-09.18.2024.jpg', 'lego-07.12.20-09.18.2024.jpg', 'lego-07.17.20-09.18.2024.jpg', 'lego-07.17.20-09.18.2024.jpg', 'lego-08.06.20-09.18.2024.jpg', 'lego-09.05.20-09.18.2024.jpg', 'lego-09.05.20-09.18.2024.jpg', 'lego-09.09.20-09.18.2024.jpg', 'lego-09.16.20-09.18.2024.jpg', 'lego-09.16.20-09.18.2024.jpg', 'lego-11.04.20-09.18.2024.jpg', 'lego-12.17.20-09.18.2024.jpg', 'lego-13.02.20-09.18.2024.jpg', 'lego-13.14.20-09.18.2024.jpg', 'lego-13.14.20-09.18.2024.jpg', 'lego-14.05.20-09.18.2024.jpg', 'lego-14.11.20-09.18.2024.jpg', 'lego-15.15.20-09.18.2024.jpg', 'lego-16.16.20-09.18.2024.jpg', 'lego-16.16.20-09.18.2024.jpg', 'lego-17.04.20-09.18.2024.jpg', 'lego-17.09.20-09.18.2024.jpg', 'lego-17.10.20-09.18.2024.jpg', 'lego-17.10.20-09.18.2024.jpg', 'lego-17.17.20-09.18.2024.jpg', 'lego-18.12.20-09.18.2024.jpg', 'lego-18.12.20-09.18.2024.jpg', 'lego-19.07.20-09.18.2024.jpg', 'lego-19.08.20-09.18.2024.jpg', 'lego-19.14.20-09.18.2024.jpg', 'lego-19.14.20-09.18.2024.jpg', 'lego-21.05.20-09.18.2024.jpg', 'lego-22.03.20-09.18.2024.jpg', 'lego-22.04.20-09.18.2024.jpg', 'lego-22.16.20-09.18.2024.jpg', 'lego-22.16.20-09.18.2024.jpg', 'lego-23.10.20-09.18.2024.jpg', 'lego-23.12.20-09.18.2024.jpg', 'lego-23.12.20-09.18.2024.jpg', 'lego-23.17.20-09.18.2024.jpg', 'lego-24.14.20-09.18.2024.jpg', 'lego-24.14.20-09.18.2024.jpg', 'lego-25.15.20-09.18.2024.jpg', 'lego-26.05.20-09.18.2024.jpg', 'lego-26.07.20-09.18.2024.jpg', 'lego-27.11.20-09.18.2024.jpg', 'lego-30.07.20-09.18.2024.jpg', 'lego-30.17.20-09.18.2024.jpg', 'lego-31.09.20-09.18.2024.jpg', 'lego-31.09.20-09.18.2024.jpg', 'lego-32.06.20-09.18.2024.jpg', 'lego-32.16.20-09.18.2024.jpg', 'lego-34.02.20-09.18.2024.jpg', 'lego-34.03.20-09.18.2024.jpg', 'lego-34.05.20-09.18.2024.jpg', 'lego-35.04.20-09.18.2024.jpg', 'lego-36.06.20-09.18.2024.jpg', 'lego-36.09.20-09.18.2024.jpg', 'lego-36.12.20-09.18.2024.jpg', 'lego-36.12.20-09.18.2024.jpg', 'lego-36.16.20-09.18.2024.jpg', 'lego-36.16.20-09.18.2024.jpg', 'lego-38.13.20-09.18.2024.jpg', 'lego-38.14.20-09.18.2024.jpg', 'lego-38.17.20-09.18.2024.jpg', 'lego-39.05.20-09.18.2024.jpg', 'lego-39.11.20-09.18.2024.jpg', 'lego-39.11.20-09.18.2024.jpg', 'lego-40.03.20-09.18.2024.jpg', 'lego-41.06.20-09.18.2024.jpg', 'lego-41.07.20-09.18.2024.jpg', 'lego-41.07.20-09.18.2024.jpg', 'lego-41.07.20-09.18.2024.jpg', 'lego-42.09.20-09.18.2024.jpg', 'lego-42.16.20-09.18.2024.jpg', 'lego-42.16.20-09.18.2024.jpg', 'lego-43.05.20-09.18.2024.jpg', 'lego-44.08.20-09.18.2024.jpg', 'lego-44.08.20-09.18.2024.jpg', 'lego-44.14.20-09.18.2024.jpg', 'lego-44.15.20-09.18.2024.jpg', 'lego-45.02.20-09.18.2024.jpg', 'lego-46.13.20-09.18.2024.jpg', 'lego-46.13.20-09.18.2024.jpg', 'lego-46.16.20-09.18.2024.jpg', 'lego-47.06.20-09.18.2024.jpg', 'lego-48.10.20-09.18.2024.jpg', 'lego-48.11.20-09.18.2024.jpg', 'lego-48.11.20-09.18.2024.jpg', 'lego-48.11.20-09.18.2024.jpg', 'lego-48.17.20-09.18.2024.jpg', 'lego-49.14.20-09.18.2024.jpg', 'lego-50.04.20-09.18.2024.jpg', 'lego-50.12.20-09.18.2024.jpg', 'lego-50.12.20-09.18.2024.jpg', 'lego-51.05.20-09.18.2024.jpg', 'lego-51.13.20-09.18.2024.jpg', 'lego-52.15.20-09.18.2024.jpg', 'lego-53.07.20-09.18.2024.jpg', 'lego-53.17.20-09.18.2024.jpg', 'lego-55.08.20-09.18.2024.jpg', 'lego-55.12.20-09.18.2024.jpg', 'lego-55.12.20-09.18.2024.jpg', 'lego-56.05.20-09.18.2024.jpg', 'lego-56.06.20-09.18.2024.jpg', 'lego-56.14.20-09.18.2024.jpg', 'lego-57.09.20-09.18.2024.jpg', 'lego-57.09.20-09.18.2024.jpg', 'lego-57.09.20-09.18.2024.jpg', 'lego-58.04.20-09.18.2024.jpg', 'lego-58.04.20-09.18.2024.jpg', 'lego-58.15.20-09.18.2024.jpg', 'lego-59.01.20-09.18.2024.jpg', 'lego-59.03.20-09.18.2024.jpg', 'lego-59.03.20-09.18.2024.jpg', 'lego-59.08.20-09.18.2024.jpg', 'lego-59.13.20-09.18.2024.jpg', 'lego-59.17.20-09.18.2024.jpg']\n",
            "\n",
            "[{'bbox': [215, 200, 313, 322], 'obj': 'Lego Friend'}, {'bbox': [57, 165, 123, 269], 'obj': 'Lego Friend'}, {'bbox': [537, 312, 639, 403], 'obj': 'Lego Friend'}, {'bbox': [266, 202, 321, 319], 'obj': 'Lego Friend'}, {'bbox': [303, 250, 369, 400], 'obj': 'Lego Friend'}, {'bbox': [393, 113, 430, 179], 'obj': 'Lego Friend'}, {'bbox': [266, 302, 353, 456], 'obj': 'Lego Friend'}, {'bbox': [371, 239, 437, 363], 'obj': 'Lego Friend'}, {'bbox': [141, 263, 240, 399], 'obj': 'Lego Friend'}, {'bbox': [318, 236, 413, 395], 'obj': 'Lego Friend'}, {'bbox': [181, 324, 275, 477], 'obj': 'Lego Friend'}, {'bbox': [294, 276, 356, 429], 'obj': 'Lego Friend'}, {'bbox': [102, 263, 214, 423], 'obj': 'Lego Friend'}, {'bbox': [199, 104, 258, 231], 'obj': 'Lego Friend'}, {'bbox': [212, 334, 324, 445], 'obj': 'Lego Friend'}, {'bbox': [413, 316, 494, 477], 'obj': 'Lego Friend'}, {'bbox': [380, 125, 432, 197], 'obj': 'Lego Friend'}, {'bbox': [127, 325, 268, 388], 'obj': 'Lego Friend'}, {'bbox': [214, 370, 333, 478], 'obj': 'Lego Friend'}, {'bbox': [219, 245, 293, 412], 'obj': 'Lego Friend'}, {'bbox': [142, 373, 263, 478], 'obj': 'Lego Friend'}, {'bbox': [278, 385, 380, 476], 'obj': 'Lego Friend'}, {'bbox': [229, 329, 344, 479], 'obj': 'Lego Friend'}, {'bbox': [395, 329, 498, 477], 'obj': 'Lego Friend'}, {'bbox': [284, 388, 367, 473], 'obj': 'Lego Friend'}, {'bbox': [109, 283, 197, 411], 'obj': 'Lego Friend'}, {'bbox': [296, 289, 369, 432], 'obj': 'Lego Friend'}, {'bbox': [123, 318, 268, 392], 'obj': 'Lego Friend'}, {'bbox': [203, 289, 295, 457], 'obj': 'Lego Friend'}, {'bbox': [280, 210, 375, 379], 'obj': 'Lego Friend'}, {'bbox': [233, 343, 334, 406], 'obj': 'Lego Friend'}, {'bbox': [352, 346, 440, 444], 'obj': 'Lego Friend'}, {'bbox': [98, 239, 182, 364], 'obj': 'Lego Friend'}, {'bbox': [300, 233, 356, 374], 'obj': 'Lego Friend'}, {'bbox': [269, 211, 320, 317], 'obj': 'Lego Friend'}, {'bbox': [280, 272, 371, 410], 'obj': 'Lego Friend'}, {'bbox': [298, 353, 407, 430], 'obj': 'Lego Friend'}, {'bbox': [200, 115, 255, 222], 'obj': 'Lego Friend'}, {'bbox': [291, 108, 340, 204], 'obj': 'Lego Friend'}, {'bbox': [206, 276, 294, 427], 'obj': 'Lego Friend'}, {'bbox': [267, 104, 326, 220], 'obj': 'Lego Friend'}, {'bbox': [306, 298, 377, 367], 'obj': 'Lego Friend'}, {'bbox': [287, 344, 369, 471], 'obj': 'Lego Friend'}, {'bbox': [228, 235, 313, 349], 'obj': 'Lego Friend'}, {'bbox': [257, 327, 357, 479], 'obj': 'Lego Friend'}, {'bbox': [345, 194, 399, 288], 'obj': 'Lego Friend'}, {'bbox': [233, 310, 325, 408], 'obj': 'Lego Friend'}, {'bbox': [347, 356, 422, 462], 'obj': 'Lego Friend'}, {'bbox': [214, 201, 298, 301], 'obj': 'Lego Friend'}, {'bbox': [199, 114, 258, 226], 'obj': 'Lego Friend'}, {'bbox': [293, 106, 339, 205], 'obj': 'Lego Friend'}, {'bbox': [190, 362, 305, 449], 'obj': 'Lego Friend'}, {'bbox': [217, 313, 301, 455], 'obj': 'Lego Friend'}, {'bbox': [306, 294, 369, 445], 'obj': 'Lego Friend'}, {'bbox': [326, 118, 381, 233], 'obj': 'Lego Friend'}, {'bbox': [242, 252, 320, 378], 'obj': 'Lego Friend'}, {'bbox': [155, 202, 218, 312], 'obj': 'Lego Friend'}, {'bbox': [196, 112, 257, 233], 'obj': 'Lego Friend'}, {'bbox': [367, 201, 426, 311], 'obj': 'Lego Friend'}, {'bbox': [401, 347, 480, 450], 'obj': 'Lego Friend'}, {'bbox': [119, 219, 195, 327], 'obj': 'Lego Friend'}, {'bbox': [267, 233, 327, 348], 'obj': 'Lego Friend'}, {'bbox': [260, 243, 330, 363], 'obj': 'Lego Friend'}, {'bbox': [342, 303, 407, 457], 'obj': 'Lego Friend'}, {'bbox': [291, 209, 345, 315], 'obj': 'Lego Friend'}, {'bbox': [0, 232, 73, 350], 'obj': 'Lego Friend'}, {'bbox': [149, 200, 206, 309], 'obj': 'Lego Friend'}, {'bbox': [258, 201, 314, 308], 'obj': 'Lego Friend'}, {'bbox': [117, 185, 171, 289], 'obj': 'Lego Friend'}, {'bbox': [121, 213, 189, 323], 'obj': 'Lego Friend'}, {'bbox': [121, 275, 198, 415], 'obj': 'Lego Friend'}, {'bbox': [199, 112, 260, 223], 'obj': 'Lego Friend'}, {'bbox': [164, 302, 245, 444], 'obj': 'Lego Friend'}, {'bbox': [345, 303, 410, 451], 'obj': 'Lego Friend'}, {'bbox': [214, 295, 288, 434], 'obj': 'Lego Friend'}, {'bbox': [245, 202, 333, 363], 'obj': 'Lego Friend'}, {'bbox': [374, 348, 461, 453], 'obj': 'Lego Friend'}, {'bbox': [281, 264, 350, 398], 'obj': 'Lego Friend'}, {'bbox': [133, 253, 204, 388], 'obj': 'Lego Friend'}, {'bbox': [200, 110, 252, 228], 'obj': 'Lego Friend'}, {'bbox': [194, 187, 253, 281], 'obj': 'Lego Friend'}, {'bbox': [371, 151, 419, 227], 'obj': 'Lego Friend'}, {'bbox': [138, 243, 225, 382], 'obj': 'Lego Friend'}, {'bbox': [279, 251, 348, 367], 'obj': 'Lego Friend'}, {'bbox': [289, 358, 362, 474], 'obj': 'Lego Friend'}, {'bbox': [209, 283, 329, 332], 'obj': 'Lego Friend'}, {'bbox': [343, 295, 402, 466], 'obj': 'Lego Friend'}, {'bbox': [231, 312, 329, 369], 'obj': 'Lego Friend'}, {'bbox': [464, 360, 564, 476], 'obj': 'Lego Friend'}, {'bbox': [233, 261, 317, 399], 'obj': 'Lego Friend'}, {'bbox': [349, 339, 425, 475], 'obj': 'Lego Friend'}, {'bbox': [213, 208, 306, 370], 'obj': 'Lego Friend'}, {'bbox': [52, 178, 135, 272], 'obj': 'Lego Friend'}, {'bbox': [279, 211, 350, 316], 'obj': 'Lego Friend'}, {'bbox': [215, 296, 289, 442], 'obj': 'Lego Friend'}, {'bbox': [211, 256, 269, 312], 'obj': 'Lego Friend'}, {'bbox': [230, 315, 333, 376], 'obj': 'Lego Friend'}, {'bbox': [247, 383, 375, 479], 'obj': 'Lego Friend'}, {'bbox': [171, 217, 247, 323], 'obj': 'Lego Friend'}, {'bbox': [145, 250, 205, 382], 'obj': 'Lego Friend'}, {'bbox': [320, 328, 387, 478], 'obj': 'Lego Friend'}, {'bbox': [204, 108, 247, 224], 'obj': 'Lego Friend'}, {'bbox': [152, 4, 204, 94], 'obj': 'Lego Friend'}, {'bbox': [304, 257, 402, 474], 'obj': 'Lego Friend'}, {'bbox': [219, 366, 328, 477], 'obj': 'Lego Friend'}, {'bbox': [127, 269, 203, 401], 'obj': 'Lego Friend'}, {'bbox': [388, 112, 432, 182], 'obj': 'Lego Friend'}, {'bbox': [375, 282, 443, 428], 'obj': 'Lego Friend'}, {'bbox': [300, 403, 402, 476], 'obj': 'Lego Friend'}, {'bbox': [279, 251, 361, 368], 'obj': 'Lego Friend'}, {'bbox': [162, 268, 226, 382], 'obj': 'Lego Friend'}, {'bbox': [332, 120, 396, 230], 'obj': 'Lego Friend'}, {'bbox': [317, 328, 404, 428], 'obj': 'Lego Friend'}, {'bbox': [143, 269, 208, 411], 'obj': 'Lego Friend'}, {'bbox': [410, 116, 437, 181], 'obj': 'Lego Friend'}, {'bbox': [212, 248, 278, 380], 'obj': 'Lego Friend'}, {'bbox': [177, 299, 248, 464], 'obj': 'Lego Friend'}, {'bbox': [487, 460, 633, 478], 'obj': 'Lego Friend'}, {'bbox': [264, 208, 317, 306], 'obj': 'Lego Friend'}, {'bbox': [67, 314, 127, 457], 'obj': 'Lego Friend'}, {'bbox': [301, 264, 363, 390], 'obj': 'Lego Friend'}, {'bbox': [131, 280, 206, 432], 'obj': 'Lego Friend'}, {'bbox': [225, 362, 315, 473], 'obj': 'Lego Friend'}, {'bbox': [320, 207, 399, 376], 'obj': 'Lego Friend'}, {'bbox': [232, 289, 336, 477], 'obj': 'Lego Friend'}, {'bbox': [195, 187, 250, 281], 'obj': 'Lego Friend'}, {'bbox': [298, 224, 362, 333], 'obj': 'Lego Friend'}, {'bbox': [151, 222, 215, 349], 'obj': 'Lego Friend'}, {'bbox': [354, 123, 421, 232], 'obj': 'Lego Friend'}, {'bbox': [381, 278, 448, 409], 'obj': 'Lego Friend'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Number of images in list: {len(images)}\")\n",
        "print(f\"Number of annotations in list: {len(annotations)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wb4xeQ7zgJ1b",
        "outputId": "0512a7c7-da7a-49e7-f91e-865d42ebcdc9"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of images in list: 130\n",
            "Number of annotations in list: 130\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Preparing our Dataset\n",
        "- Splitting data into training and testing sets"
      ],
      "metadata": {
        "id": "-sLHvcsxhbio"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "ELAd7H2Fhp-X"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_images, validate_images, train_annotations, validate_annotations = train_test_split(images, annotations, test_size=0.2, random_state=1)"
      ],
      "metadata": {
        "id": "3pPmbfxYhvs4"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Number of training images: {len(train_images)} and number of training annotations: {len(train_annotations)}\")\n",
        "print(f\"Number of validation images: {len(validate_images)} and number of validation annotations: {len(validate_annotations)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H7kolojvh8hb",
        "outputId": "371dfe34-56b3-47cb-9ddd-39ac0e1f19f8"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training images: 104 and number of training annotations: 104\n",
            "Number of validation images: 26 and number of validation annotations: 26\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Convert Annotations to Tensorflow Format\n",
        "- Want to use tf.data.Dataset, so we need to reformat our data to fit into this"
      ],
      "metadata": {
        "id": "bv94W3SVjEg7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "1cTSJttDfJRe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_tf_data(image, annotations):\n",
        "  \"\"\"\n",
        "  Takes the image and the annotations parsed from json file and converts it to an \"example,\"\n",
        "  which is what tf.data.Dataset needs to be fed. We're serializing the data so we can put it into\n",
        "  tf.data.Dataset and thus use it later.\n",
        "  \"\"\"\n",
        "  image_path = f'/content/drive/MyDrive/lego_men_photos/{image}'\n",
        "  image_string = tf.io.read_file(image_path)\n",
        "  img = tf.image.decode_jpeg(image_string, channels=3)\n",
        "\n",
        "  # Identifying the bounding box and object name in annotations\n",
        "  bbx = annotations['bbox']\n",
        "  obj_name = annotations['obj']\n",
        "\n",
        "  # Creating tf.train.Example\n",
        "  tf_example = tf.train.Example(features=tf.train.Features(feature={\n",
        "      'image/encoded': tf.train.Feature(bytes_list=tf.train.BytesList(value=[image_string.numpy()])),\n",
        "      # We appended data into bbox as 'bbox': [xmin, ymin, xmax, ymax], so we must label it in this order, too\n",
        "      'image/object/bbox/xmin': tf.train.Feature(float_list=tf.train.FloatList(value=[bbx[0]])),\n",
        "      'image/object/bbox/ymin': tf.train.Feature(float_list=tf.train.FloatList(value=[bbx[1]])),\n",
        "      'image/object/bbox/xmax': tf.train.Feature(float_list=tf.train.FloatList(value=[bbx[2]])),\n",
        "      'image/object/bbox/ymax': tf.train.Feature(float_list=tf.train.FloatList(value=[bbx[3]])),\n",
        "      'image/object/class/text': tf.train.Feature(bytes_list=tf.train.BytesList(value=[obj_name.encode('utf8')])),\n",
        "  }))\n",
        "\n",
        "  return tf_example\n",
        "\n",
        "\n",
        "# Using list comprehension to run all training data (images and annotations) through creat_df_data function\n",
        "train_tfrecords = [create_tf_data(train_images[i], train_annotations[i]) for i in range(len(train_images))]\n",
        "\n",
        "# Using list comprehension to run all validation data through, too\n",
        "validate_tfrecords = [create_tf_data(validate_images[i], validate_annotations[i]) for i in range(len(validate_images))]\n",
        "\n",
        "# Write training examples to TFRecord files\n",
        "with tf.io.TFRecordWriter('training.record')as writer:\n",
        "  for example in train_tfrecords:\n",
        "    writer.write(example.SerializeToString())\n",
        "\n",
        "# Write validation examples to TFRecord files\n",
        "with tf.io.TFRecordWriter('validation.record') as writer:\n",
        "  for example in validate_tfrecords:\n",
        "    writer.write(example.SerializeToString())\n"
      ],
      "metadata": {
        "id": "Rd1dmqH9jMJd"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Coding Notes\n",
        "- It's considered best practice to have a comma at the end of a value-key pair, since it makes it easier to add / remove lines later manually and with version control\n",
        "- We use value=[bbx[3]] and not value=bbx[3] because \"Tensorflow FloatList expects its input to be a list,\" according to ChatGPT."
      ],
      "metadata": {
        "id": "pE2d3R6bob4-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have the training and validation records to use later with Tensorflow."
      ],
      "metadata": {
        "id": "yMvGKrtKtQWM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Model Training\n",
        "(summary of next instructions from ChatGPT)\n",
        "- Using TensorFlow's Object Detection API\n",
        "- Must choose a model (here we'll use Coco)\n",
        "- Edit config file\n",
        "  - Set paths for training and validation files (we just made these)\n",
        "  -   Adjust the number of classes (here we have 1: \"Lego Friend\")\n",
        "  - Set batch size, learning rate, ect."
      ],
      "metadata": {
        "id": "BKoMEA4OtUEe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download Tensorflow and Tensorflow Object Detection API\n",
        "- Using Colab like a terminal now\n",
        "  - Use a lot of \"!\"'s to signify that we're typing bash/command terminal commands"
      ],
      "metadata": {
        "id": "wdQ1u-lRt3Vd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/tensorflow/models.git = TensorFlow Model Garden (can pick models out to use from here)\n",
        "- We'll be using the API"
      ],
      "metadata": {
        "id": "6tUO4MRAuYKN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install tensorflow\n",
        "!pip install tensorflow==2.8"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "u2N3PJIZvxXg",
        "outputId": "95a1cbf0-804c-4b2e-9d6b-9f67b9250943"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow==2.8\n",
            "  Downloading tensorflow-2.8.0-cp310-cp310-manylinux2010_x86_64.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (24.3.25)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (3.11.0)\n",
            "Collecting keras-preprocessing>=1.1.1 (from tensorflow==2.8)\n",
            "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (18.1.1)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (1.26.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (3.4.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (71.0.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (1.16.0)\n",
            "Collecting tensorboard<2.9,>=2.8 (from tensorflow==2.8)\n",
            "  Downloading tensorboard-2.8.0-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting tf-estimator-nightly==2.8.0.dev2021122109 (from tensorflow==2.8)\n",
            "  Downloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting keras<2.9,>=2.8.0rc0 (from tensorflow==2.8)\n",
            "  Downloading keras-2.8.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (0.37.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (1.64.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.8) (0.44.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8) (2.27.0)\n",
            "Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.9,>=2.8->tensorflow==2.8)\n",
            "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8) (3.7)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8) (2.32.3)\n",
            "Collecting tensorboard-data-server<0.7.0,>=0.6.0 (from tensorboard<2.9,>=2.8->tensorflow==2.8)\n",
            "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl.metadata (1.1 kB)\n",
            "Collecting tensorboard-plugin-wit>=1.6.0 (from tensorboard<2.9,>=2.8->tensorflow==2.8)\n",
            "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl.metadata (873 bytes)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8) (3.0.4)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow==2.8) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=0.11.15->tensorboard<2.9,>=2.8->tensorflow==2.8) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow==2.8) (3.2.2)\n",
            "Downloading tensorflow-2.8.0-cp310-cp310-manylinux2010_x86_64.whl (497.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m497.6/497.6 MB\u001b[0m \u001b[31m771.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.5/462.5 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading keras-2.8.0-py2.py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m51.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.8.0-py3-none-any.whl (5.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m72.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
            "Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m64.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.3/781.3 kB\u001b[0m \u001b[31m40.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tf-estimator-nightly, tensorboard-plugin-wit, keras, tensorboard-data-server, keras-preprocessing, google-auth-oauthlib, tensorboard, tensorflow\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 3.4.1\n",
            "    Uninstalling keras-3.4.1:\n",
            "      Successfully uninstalled keras-3.4.1\n",
            "  Attempting uninstall: tensorboard-data-server\n",
            "    Found existing installation: tensorboard-data-server 0.7.2\n",
            "    Uninstalling tensorboard-data-server-0.7.2:\n",
            "      Successfully uninstalled tensorboard-data-server-0.7.2\n",
            "  Attempting uninstall: google-auth-oauthlib\n",
            "    Found existing installation: google-auth-oauthlib 1.2.1\n",
            "    Uninstalling google-auth-oauthlib-1.2.1:\n",
            "      Successfully uninstalled google-auth-oauthlib-1.2.1\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.17.0\n",
            "    Uninstalling tensorboard-2.17.0:\n",
            "      Successfully uninstalled tensorboard-2.17.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.17.0\n",
            "    Uninstalling tensorflow-2.17.0:\n",
            "      Successfully uninstalled tensorflow-2.17.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pandas-gbq 0.23.2 requires google-auth-oauthlib>=0.7.0, but you have google-auth-oauthlib 0.4.6 which is incompatible.\n",
            "tensorflow-text 2.17.0 requires tensorflow<2.18,>=2.17.0, but you have tensorflow 2.8.0 which is incompatible.\n",
            "tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.8.0 which is incompatible.\n",
            "tf-models-official 2.17.0 requires tensorflow~=2.17.0, but you have tensorflow 2.8.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed google-auth-oauthlib-0.4.6 keras-2.8.0 keras-preprocessing-1.1.2 tensorboard-2.8.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.8.0 tf-estimator-nightly-2.8.0.dev2021122109\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google_auth_oauthlib",
                  "keras",
                  "tensorflow"
                ]
              },
              "id": "079fc8b1357a49489f347daeb2756d2f"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cloning TensorFlow models repo from github (including API)\n",
        "!git clone https://github.com/tensorflow/models.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j9WFvuOQvyzI",
        "outputId": "60b402ce-338f-43f4-c816-9dffa131c7c1"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'models' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Protobuf Installation & Compilation"
      ],
      "metadata": {
        "id": "UJgGUjh7xoym"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reference for a lot of the code here:\n",
        "https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/install.html"
      ],
      "metadata": {
        "id": "N_oat3NmwRqV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Installing the obj. detection API\n",
        "!apt-get install protobuf-compiler"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5D71aTkWv0Qd",
        "outputId": "a88facd0-12f3-4c65-af76-71e7275a77bf"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "protobuf-compiler is already the newest version (3.12.4-1ubuntu7.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 49 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Changing directory to models/research && downloading and compiling the protobuf libraries\n",
        "!cd models/research && protoc object_detection/protos/*.proto --python_out=.\n",
        "# !cp object_detection/packages/tf2/setup.py .\n",
        "# !python -m pip install ."
      ],
      "metadata": {
        "id": "62wAKQWqv188"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## COCO API Installation\n",
        "- Still following instructions from above link\n",
        "- COCO API: https://github.com/cocodataset/cocoapi.git"
      ],
      "metadata": {
        "id": "XKKv8QMBx4Uk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading cocoapi\n",
        "!git clone https://github.com/cocodataset/cocoapi.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dlWEkasyxhaE",
        "outputId": "18698a29-1488-4f39-b8c0-a806e8ac8e9b"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'cocoapi'...\n",
            "remote: Enumerating objects: 975, done.\u001b[K\n",
            "remote: Total 975 (delta 0), reused 0 (delta 0), pack-reused 975 (from 1)\u001b[K\n",
            "Receiving objects: 100% (975/975), 11.72 MiB | 17.40 MiB/s, done.\n",
            "Resolving deltas: 100% (576/576), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Making the cocoapi repo\n",
        "!cd cocoapi/PythonAPI && make"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TIBCIeb5ySXd",
        "outputId": "df8664b8-898b-45c4-e2bb-d588a7626b0f"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "python setup.py build_ext --inplace\n",
            "running build_ext\n",
            "Compiling pycocotools/_mask.pyx because it changed.\n",
            "[1/1] Cythonizing pycocotools/_mask.pyx\n",
            "/usr/local/lib/python3.10/dist-packages/Cython/Compiler/Main.py:381: FutureWarning: Cython directive 'language_level' not set, using '3str' for now (Py3). This has changed from earlier releases! File: /content/cocoapi/PythonAPI/pycocotools/_mask.pyx\n",
            "  tree = Parsing.p_module(s, pxd, full_module_name)\n",
            "building 'pycocotools._mask' extension\n",
            "creating build\n",
            "creating build/common\n",
            "creating build/temp.linux-x86_64-cpython-310\n",
            "creating build/temp.linux-x86_64-cpython-310/pycocotools\n",
            "x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/local/lib/python3.10/dist-packages/numpy/core/include -I../common -I/usr/include/python3.10 -c ../common/maskApi.c -o build/temp.linux-x86_64-cpython-310/../common/maskApi.o -Wno-cpp -Wno-unused-function -std=c99\n",
            "\u001b[01m\u001b[K../common/maskApi.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[KrleDecode\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K../common/maskApi.c:46:7:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kthis ‘\u001b[01m\u001b[Kfor\u001b[m\u001b[K’ clause does not guard... [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wmisleading-indentation\u0007-Wmisleading-indentation\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "   46 |       \u001b[01;35m\u001b[Kfor\u001b[m\u001b[K( k=0; k<R[i].cnts[j]; k++ ) *(M++)=v; v=!v; }}\n",
            "      |       \u001b[01;35m\u001b[K^~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K../common/maskApi.c:46:49:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K...this statement, but the latter is misleadingly indented as if it were guarded by the ‘\u001b[01m\u001b[Kfor\u001b[m\u001b[K’\n",
            "   46 |       for( k=0; k<R[i].cnts[j]; k++ ) *(M++)=v; \u001b[01;36m\u001b[Kv\u001b[m\u001b[K=!v; }}\n",
            "      |                                                 \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K../common/maskApi.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[KrleFrPoly\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K../common/maskApi.c:166:3:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kthis ‘\u001b[01m\u001b[Kfor\u001b[m\u001b[K’ clause does not guard... [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wmisleading-indentation\u0007-Wmisleading-indentation\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  166 |   \u001b[01;35m\u001b[Kfor\u001b[m\u001b[K(j=0; j<k; j++) x[j]=(int)(scale*xy[j*2+0]+.5); x[k]=x[0];\n",
            "      |   \u001b[01;35m\u001b[K^~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K../common/maskApi.c:166:54:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K...this statement, but the latter is misleadingly indented as if it were guarded by the ‘\u001b[01m\u001b[Kfor\u001b[m\u001b[K’\n",
            "  166 |   for(j=0; j<k; j++) x[j]=(int)(scale*xy[j*2+0]+.5); \u001b[01;36m\u001b[Kx\u001b[m\u001b[K[k]=x[0];\n",
            "      |                                                      \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K../common/maskApi.c:167:3:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kthis ‘\u001b[01m\u001b[Kfor\u001b[m\u001b[K’ clause does not guard... [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wmisleading-indentation\u0007-Wmisleading-indentation\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  167 |   \u001b[01;35m\u001b[Kfor\u001b[m\u001b[K(j=0; j<k; j++) y[j]=(int)(scale*xy[j*2+1]+.5); y[k]=y[0];\n",
            "      |   \u001b[01;35m\u001b[K^~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K../common/maskApi.c:167:54:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K...this statement, but the latter is misleadingly indented as if it were guarded by the ‘\u001b[01m\u001b[Kfor\u001b[m\u001b[K’\n",
            "  167 |   for(j=0; j<k; j++) y[j]=(int)(scale*xy[j*2+1]+.5); \u001b[01;36m\u001b[Ky\u001b[m\u001b[K[k]=y[0];\n",
            "      |                                                      \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K../common/maskApi.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[KrleToString\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K../common/maskApi.c:212:7:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kthis ‘\u001b[01m\u001b[Kif\u001b[m\u001b[K’ clause does not guard... [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wmisleading-indentation\u0007-Wmisleading-indentation\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  212 |       \u001b[01;35m\u001b[Kif\u001b[m\u001b[K(more) c |= 0x20; c+=48; s[p++]=c;\n",
            "      |       \u001b[01;35m\u001b[K^~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K../common/maskApi.c:212:27:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K...this statement, but the latter is misleadingly indented as if it were guarded by the ‘\u001b[01m\u001b[Kif\u001b[m\u001b[K’\n",
            "  212 |       if(more) c |= 0x20; \u001b[01;36m\u001b[Kc\u001b[m\u001b[K+=48; s[p++]=c;\n",
            "      |                           \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K../common/maskApi.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[KrleFrString\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K../common/maskApi.c:220:3:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kthis ‘\u001b[01m\u001b[Kwhile\u001b[m\u001b[K’ clause does not guard... [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wmisleading-indentation\u0007-Wmisleading-indentation\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  220 |   \u001b[01;35m\u001b[Kwhile\u001b[m\u001b[K( s[m] ) m++; cnts=malloc(sizeof(uint)*m); m=0;\n",
            "      |   \u001b[01;35m\u001b[K^~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K../common/maskApi.c:220:22:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K...this statement, but the latter is misleadingly indented as if it were guarded by the ‘\u001b[01m\u001b[Kwhile\u001b[m\u001b[K’\n",
            "  220 |   while( s[m] ) m++; \u001b[01;36m\u001b[Kcnts\u001b[m\u001b[K=malloc(sizeof(uint)*m); m=0;\n",
            "      |                      \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K../common/maskApi.c:228:5:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kthis ‘\u001b[01m\u001b[Kif\u001b[m\u001b[K’ clause does not guard... [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wmisleading-indentation\u0007-Wmisleading-indentation\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  228 |     \u001b[01;35m\u001b[Kif\u001b[m\u001b[K(m>2) x+=(long) cnts[m-2]; cnts[m++]=(uint) x;\n",
            "      |     \u001b[01;35m\u001b[K^~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K../common/maskApi.c:228:34:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K...this statement, but the latter is misleadingly indented as if it were guarded by the ‘\u001b[01m\u001b[Kif\u001b[m\u001b[K’\n",
            "  228 |     if(m>2) x+=(long) cnts[m-2]; \u001b[01;36m\u001b[Kcnts\u001b[m\u001b[K[m++]=(uint) x;\n",
            "      |                                  \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/local/lib/python3.10/dist-packages/numpy/core/include -I../common -I/usr/include/python3.10 -c pycocotools/_mask.c -o build/temp.linux-x86_64-cpython-310/pycocotools/_mask.o -Wno-cpp -Wno-unused-function -std=c99\n",
            "creating build/lib.linux-x86_64-cpython-310\n",
            "creating build/lib.linux-x86_64-cpython-310/pycocotools\n",
            "x86_64-linux-gnu-gcc -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 build/temp.linux-x86_64-cpython-310/../common/maskApi.o build/temp.linux-x86_64-cpython-310/pycocotools/_mask.o -L/usr/lib/x86_64-linux-gnu -o build/lib.linux-x86_64-cpython-310/pycocotools/_mask.cpython-310-x86_64-linux-gnu.so\n",
            "copying build/lib.linux-x86_64-cpython-310/pycocotools/_mask.cpython-310-x86_64-linux-gnu.so -> pycocotools\n",
            "rm -rf build\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !cp -r pycocotools /content/models/research/ # Error...\n",
        "!cp -r cocoapi /content/models/research/"
      ],
      "metadata": {
        "id": "K0OsG32zyhmK"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Object Detection API\n",
        "- Still following instructions in above link"
      ],
      "metadata": {
        "id": "wOHAH-tUzLvs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !cd models/research && protoc object_detection/protos/*.proto --python_out=.\n",
        "# !cp object_detection/packages/tf2/setup.py .\n",
        "# !python -m pip install .\n",
        "!cd models/research && cp object_detection/packages/tf2/setup.py . && python -m pip install .\n",
        "\n",
        "#!python -m pip install . # Doesn't like this because it forgets we're in models/research! Put in above line!"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rAbacYXJzRgy",
        "outputId": "4230ae5c-1297-4f5c-9d89-ee20dec5b1ad"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing /content/models/research\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting avro-python3 (from object_detection==0.1)\n",
            "  Downloading avro-python3-1.10.2.tar.gz (38 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting apache-beam (from object_detection==0.1)\n",
            "  Downloading apache_beam-2.59.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.5 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from object_detection==0.1) (10.4.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from object_detection==0.1) (4.9.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from object_detection==0.1) (3.7.1)\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.10/dist-packages (from object_detection==0.1) (3.0.11)\n",
            "Requirement already satisfied: contextlib2 in /usr/local/lib/python3.10/dist-packages (from object_detection==0.1) (21.6.0)\n",
            "Requirement already satisfied: tf-slim in /usr/local/lib/python3.10/dist-packages (from object_detection==0.1) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from object_detection==0.1) (1.16.0)\n",
            "Requirement already satisfied: pycocotools in /usr/local/lib/python3.10/dist-packages (from object_detection==0.1) (2.0.8)\n",
            "Collecting lvis (from object_detection==0.1)\n",
            "  Downloading lvis-0.5.3-py3-none-any.whl.metadata (856 bytes)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from object_detection==0.1) (1.13.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from object_detection==0.1) (2.2.2)\n",
            "Collecting tf-models-official>=2.5.1 (from object_detection==0.1)\n",
            "  Downloading tf_models_official-2.17.0-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting tensorflow_io (from object_detection==0.1)\n",
            "  Downloading tensorflow_io-0.37.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (from object_detection==0.1) (3.4.1)\n",
            "Collecting pyparsing==2.4.7 (from object_detection==0.1)\n",
            "  Downloading pyparsing-2.4.7-py2.py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting sacrebleu<=2.2.0 (from object_detection==0.1)\n",
            "  Downloading sacrebleu-2.2.0-py3-none-any.whl.metadata (55 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.4/55.4 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting portalocker (from sacrebleu<=2.2.0->object_detection==0.1)\n",
            "  Downloading portalocker-2.10.1-py3-none-any.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu<=2.2.0->object_detection==0.1) (2024.9.11)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu<=2.2.0->object_detection==0.1) (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from sacrebleu<=2.2.0->object_detection==0.1) (1.26.4)\n",
            "Collecting colorama (from sacrebleu<=2.2.0->object_detection==0.1)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: gin-config in /usr/local/lib/python3.10/dist-packages (from tf-models-official>=2.5.1->object_detection==0.1) (0.5.0)\n",
            "Requirement already satisfied: google-api-python-client>=1.6.7 in /usr/local/lib/python3.10/dist-packages (from tf-models-official>=2.5.1->object_detection==0.1) (2.137.0)\n",
            "Requirement already satisfied: immutabledict in /usr/local/lib/python3.10/dist-packages (from tf-models-official>=2.5.1->object_detection==0.1) (4.2.0)\n",
            "Requirement already satisfied: kaggle>=1.3.9 in /usr/local/lib/python3.10/dist-packages (from tf-models-official>=2.5.1->object_detection==0.1) (1.6.17)\n",
            "Requirement already satisfied: oauth2client in /usr/local/lib/python3.10/dist-packages (from tf-models-official>=2.5.1->object_detection==0.1) (4.1.3)\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.10/dist-packages (from tf-models-official>=2.5.1->object_detection==0.1) (4.10.0.84)\n",
            "Requirement already satisfied: psutil>=5.4.3 in /usr/local/lib/python3.10/dist-packages (from tf-models-official>=2.5.1->object_detection==0.1) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from tf-models-official>=2.5.1->object_detection==0.1) (9.0.0)\n",
            "Requirement already satisfied: pyyaml>=6.0.0 in /usr/local/lib/python3.10/dist-packages (from tf-models-official>=2.5.1->object_detection==0.1) (6.0.2)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from tf-models-official>=2.5.1->object_detection==0.1) (0.2.0)\n",
            "Collecting seqeval (from tf-models-official>=2.5.1->object_detection==0.1)\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.10/dist-packages (from tf-models-official>=2.5.1->object_detection==0.1) (4.9.6)\n",
            "Requirement already satisfied: tensorflow-hub>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from tf-models-official>=2.5.1->object_detection==0.1) (0.16.1)\n",
            "Collecting tensorflow-model-optimization>=0.4.1 (from tf-models-official>=2.5.1->object_detection==0.1)\n",
            "  Downloading tensorflow_model_optimization-0.8.0-py2.py3-none-any.whl.metadata (904 bytes)\n",
            "Collecting tensorflow-text~=2.17.0 (from tf-models-official>=2.5.1->object_detection==0.1)\n",
            "  Downloading tensorflow_text-2.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: tensorflow~=2.17.0 in /usr/local/lib/python3.10/dist-packages (from tf-models-official>=2.5.1->object_detection==0.1) (2.17.0)\n",
            "Requirement already satisfied: tf-keras>=2.16.0 in /usr/local/lib/python3.10/dist-packages (from tf-models-official>=2.5.1->object_detection==0.1) (2.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->object_detection==0.1) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->object_detection==0.1) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->object_detection==0.1) (2024.2)\n",
            "Requirement already satisfied: absl-py>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from tf-slim->object_detection==0.1) (1.4.0)\n",
            "Collecting crcmod<2.0,>=1.7 (from apache-beam->object_detection==0.1)\n",
            "  Downloading crcmod-1.7.tar.gz (89 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.7/89.7 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting orjson<4,>=3.9.7 (from apache-beam->object_detection==0.1)\n",
            "  Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dill<0.3.2,>=0.3.1.1 (from apache-beam->object_detection==0.1)\n",
            "  Downloading dill-0.3.1.1.tar.gz (151 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.0/152.0 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cloudpickle~=2.2.1 in /usr/local/lib/python3.10/dist-packages (from apache-beam->object_detection==0.1) (2.2.1)\n",
            "Collecting fastavro<2,>=0.23.6 (from apache-beam->object_detection==0.1)\n",
            "  Downloading fastavro-1.9.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
            "Collecting fasteners<1.0,>=0.3 (from apache-beam->object_detection==0.1)\n",
            "  Downloading fasteners-0.19-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: grpcio!=1.48.0,!=1.59.*,!=1.60.*,!=1.61.*,!=1.62.0,!=1.62.1,<2,>=1.33.1 in /usr/local/lib/python3.10/dist-packages (from apache-beam->object_detection==0.1) (1.64.1)\n",
            "Collecting hdfs<3.0.0,>=2.1.0 (from apache-beam->object_detection==0.1)\n",
            "  Downloading hdfs-2.7.3.tar.gz (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: httplib2<0.23.0,>=0.8 in /usr/local/lib/python3.10/dist-packages (from apache-beam->object_detection==0.1) (0.22.0)\n",
            "Requirement already satisfied: jsonschema<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam->object_detection==0.1) (4.23.0)\n",
            "Requirement already satisfied: jsonpickle<4.0.0,>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam->object_detection==0.1) (3.3.0)\n",
            "Collecting objsize<0.8.0,>=0.6.1 (from apache-beam->object_detection==0.1)\n",
            "  Downloading objsize-0.7.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: packaging>=22.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam->object_detection==0.1) (24.1)\n",
            "Collecting pymongo<5.0.0,>=3.8.0 (from apache-beam->object_detection==0.1)\n",
            "  Downloading pymongo-4.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)\n",
            "Requirement already satisfied: proto-plus<2,>=1.7.1 in /usr/local/lib/python3.10/dist-packages (from apache-beam->object_detection==0.1) (1.24.0)\n",
            "Requirement already satisfied: protobuf!=4.0.*,!=4.21.*,!=4.22.0,!=4.23.*,!=4.24.*,<4.26.0,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from apache-beam->object_detection==0.1) (3.20.3)\n",
            "Collecting pydot<2,>=1.2.0 (from apache-beam->object_detection==0.1)\n",
            "  Downloading pydot-1.4.2-py2.py3-none-any.whl.metadata (8.0 kB)\n",
            "Collecting redis<6,>=5.0.0 (from apache-beam->object_detection==0.1)\n",
            "  Downloading redis-5.1.1-py3-none-any.whl.metadata (9.1 kB)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.24.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam->object_detection==0.1) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam->object_detection==0.1) (4.12.2)\n",
            "Collecting zstandard<1,>=0.18.0 (from apache-beam->object_detection==0.1)\n",
            "  Downloading zstandard-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: pyarrow<17.0.0,>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam->object_detection==0.1) (16.1.0)\n",
            "Requirement already satisfied: pyarrow-hotfix<1 in /usr/local/lib/python3.10/dist-packages (from apache-beam->object_detection==0.1) (0.6)\n",
            "Collecting js2py<1,>=0.74 (from apache-beam->object_detection==0.1)\n",
            "  Downloading Js2Py-0.74-py3-none-any.whl.metadata (868 bytes)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras->object_detection==0.1) (13.8.1)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras->object_detection==0.1) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras->object_detection==0.1) (3.11.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras->object_detection==0.1) (0.12.1)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras->object_detection==0.1) (0.4.1)\n",
            "Requirement already satisfied: cycler>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from lvis->object_detection==0.1) (0.12.1)\n",
            "Requirement already satisfied: kiwisolver>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from lvis->object_detection==0.1) (1.4.7)\n",
            "Requirement already satisfied: opencv-python>=4.1.0.25 in /usr/local/lib/python3.10/dist-packages (from lvis->object_detection==0.1) (4.10.0.84)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->object_detection==0.1) (1.3.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->object_detection==0.1) (4.54.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem==0.37.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow_io->object_detection==0.1) (0.37.1)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object_detection==0.1) (2.27.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object_detection==0.1) (0.2.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object_detection==0.1) (2.19.2)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object_detection==0.1) (4.1.1)\n",
            "Collecting docopt (from hdfs<3.0.0,>=2.1.0->apache-beam->object_detection==0.1)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tzlocal>=1.2 in /usr/local/lib/python3.10/dist-packages (from js2py<1,>=0.74->apache-beam->object_detection==0.1) (5.2)\n",
            "Collecting pyjsparser>=2.5.1 (from js2py<1,>=0.74->apache-beam->object_detection==0.1)\n",
            "  Downloading pyjsparser-2.7.1.tar.gz (24 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.0.0->apache-beam->object_detection==0.1) (24.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.0.0->apache-beam->object_detection==0.1) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.0.0->apache-beam->object_detection==0.1) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.0.0->apache-beam->object_detection==0.1) (0.20.0)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.10/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object_detection==0.1) (2024.8.30)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object_detection==0.1) (4.66.5)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object_detection==0.1) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object_detection==0.1) (2.2.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object_detection==0.1) (6.1.0)\n",
            "Collecting dnspython<3.0.0,>=1.16.0 (from pymongo<5.0.0,>=3.8.0->apache-beam->object_detection==0.1)\n",
            "  Downloading dnspython-2.6.1-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: async-timeout>=4.0.3 in /usr/local/lib/python3.10/dist-packages (from redis<6,>=5.0.0->apache-beam->object_detection==0.1) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam->object_detection==0.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam->object_detection==0.1) (3.10)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.17.0->tf-models-official>=2.5.1->object_detection==0.1) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.17.0->tf-models-official>=2.5.1->object_detection==0.1) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.17.0->tf-models-official>=2.5.1->object_detection==0.1) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.17.0->tf-models-official>=2.5.1->object_detection==0.1) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.17.0->tf-models-official>=2.5.1->object_detection==0.1) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.17.0->tf-models-official>=2.5.1->object_detection==0.1) (3.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.17.0->tf-models-official>=2.5.1->object_detection==0.1) (71.0.4)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.17.0->tf-models-official>=2.5.1->object_detection==0.1) (2.4.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.17.0->tf-models-official>=2.5.1->object_detection==0.1) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.17.0->tf-models-official>=2.5.1->object_detection==0.1) (2.17.0)\n",
            "Requirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow-model-optimization>=0.4.1->tf-models-official>=2.5.1->object_detection==0.1) (0.1.8)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.10/dist-packages (from oauth2client->tf-models-official>=2.5.1->object_detection==0.1) (0.6.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.10/dist-packages (from oauth2client->tf-models-official>=2.5.1->object_detection==0.1) (0.4.1)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from oauth2client->tf-models-official>=2.5.1->object_detection==0.1) (4.9)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras->object_detection==0.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras->object_detection==0.1) (2.18.0)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.10/dist-packages (from seqeval->tf-models-official>=2.5.1->object_detection==0.1) (1.5.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object_detection==0.1) (8.1.7)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object_detection==0.1) (2.3)\n",
            "Requirement already satisfied: simple-parsing in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object_detection==0.1) (0.1.6)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object_detection==0.1) (1.15.0)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object_detection==0.1) (0.10.2)\n",
            "Requirement already satisfied: array-record>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object_detection==0.1) (0.5.1)\n",
            "Requirement already satisfied: etils>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow-datasets->tf-models-official>=2.5.1->object_detection==0.1) (1.9.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow~=2.17.0->tf-models-official>=2.5.1->object_detection==0.1) (0.44.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow-datasets->tf-models-official>=2.5.1->object_detection==0.1) (2024.6.1)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow-datasets->tf-models-official>=2.5.1->object_detection==0.1) (6.4.5)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow-datasets->tf-models-official>=2.5.1->object_detection==0.1) (3.20.2)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object_detection==0.1) (1.65.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object_detection==0.1) (5.5.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras->object_detection==0.1) (0.1.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official>=2.5.1->object_detection==0.1) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official>=2.5.1->object_detection==0.1) (3.5.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow~=2.17.0->tf-models-official>=2.5.1->object_detection==0.1) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow~=2.17.0->tf-models-official>=2.5.1->object_detection==0.1) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow~=2.17.0->tf-models-official>=2.5.1->object_detection==0.1) (3.0.4)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle>=1.3.9->tf-models-official>=2.5.1->object_detection==0.1) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle>=1.3.9->tf-models-official>=2.5.1->object_detection==0.1) (1.3)\n",
            "Requirement already satisfied: docstring-parser<1.0,>=0.15 in /usr/local/lib/python3.10/dist-packages (from simple-parsing->tensorflow-datasets->tf-models-official>=2.5.1->object_detection==0.1) (0.16)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow~=2.17.0->tf-models-official>=2.5.1->object_detection==0.1) (2.1.5)\n",
            "Downloading pyparsing-2.4.7-py2.py3-none-any.whl (67 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.8/67.8 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sacrebleu-2.2.0-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.6/116.6 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tf_models_official-2.17.0-py2.py3-none-any.whl (2.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m38.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading apache_beam-2.59.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.6/15.6 MB\u001b[0m \u001b[31m43.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lvis-0.5.3-py3-none-any.whl (14 kB)\n",
            "Downloading tensorflow_io-0.37.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.6/49.6 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastavro-1.9.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m58.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fasteners-0.19-py3-none-any.whl (18 kB)\n",
            "Downloading Js2Py-0.74-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m45.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading objsize-0.7.0-py3-none-any.whl (11 kB)\n",
            "Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydot-1.4.2-py2.py3-none-any.whl (21 kB)\n",
            "Downloading pymongo-4.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m57.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading redis-5.1.1-py3-none-any.whl (261 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m261.3/261.3 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_model_optimization-0.8.0-py2.py3-none-any.whl (242 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.5/242.5 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_text-2.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m76.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading zstandard-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m80.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading portalocker-2.10.1-py3-none-any.whl (18 kB)\n",
            "Downloading dnspython-2.6.1-py3-none-any.whl (307 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: object_detection, avro-python3, crcmod, dill, hdfs, seqeval, pyjsparser, docopt\n",
            "  Building wheel for object_detection (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for object_detection: filename=object_detection-0.1-py3-none-any.whl size=1697358 sha256=8a9e3f54f7c34c4f804abe07cba1e6dfb7627b5d7fdb2af4a0fd894bf7d93da8\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-ekp99fag/wheels/53/dd/70/2de274d6c443c69d367bd6a5606f95e5a6df61aacf1435ec0d\n",
            "  Building wheel for avro-python3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for avro-python3: filename=avro_python3-1.10.2-py3-none-any.whl size=43994 sha256=5aa81b846bd30ec0cd7b92aae1803bae5b0d71930326a8d1ac4fde2e0cd169f4\n",
            "  Stored in directory: /root/.cache/pip/wheels/bc/85/62/6cdd81c56f923946b401cecff38055b94c9b766927f7d8ca82\n",
            "  Building wheel for crcmod (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for crcmod: filename=crcmod-1.7-cp310-cp310-linux_x86_64.whl size=31404 sha256=cc543b928311683d729e40c90b19f6969ec60d03dc851d6729c91948a3a48601\n",
            "  Stored in directory: /root/.cache/pip/wheels/85/4c/07/72215c529bd59d67e3dac29711d7aba1b692f543c808ba9e86\n",
            "  Building wheel for dill (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for dill: filename=dill-0.3.1.1-py3-none-any.whl size=78542 sha256=65fd1006cd0d0a5f3e4539ad7cb8febc6d034efbafc5db0dee68c96edc5dafeb\n",
            "  Stored in directory: /root/.cache/pip/wheels/ea/e2/86/64980d90e297e7bf2ce588c2b96e818f5399c515c4bb8a7e4f\n",
            "  Building wheel for hdfs (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hdfs: filename=hdfs-2.7.3-py3-none-any.whl size=34325 sha256=1de3dc29792780b843fe841b243d07b051fe6fd43c795889e6de786a9feaafb3\n",
            "  Stored in directory: /root/.cache/pip/wheels/e5/8d/b6/99c1c0a3ac5788c866b0ecd3f48b0134a5910e6ed26011800b\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16161 sha256=768898203fe28eb934fd5a7655658dbca2f34ee46eee650ef3e14e659bd86e00\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/67/4a/ad4082dd7dfc30f2abfe4d80a2ed5926a506eb8a972b4767fa\n",
            "  Building wheel for pyjsparser (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyjsparser: filename=pyjsparser-2.7.1-py3-none-any.whl size=25983 sha256=210738905d63b148b8544ef6f65d9aca174334584d96595388e0597ef37bbe58\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/81/26/5956478df303e2bf5a85a5df595bb307bd25948a4bab69f7c7\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13704 sha256=8582fd7cf5909f79ccdbbb77ea7e5a45419a16149d0469dd1801bd5c543d4147\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "Successfully built object_detection avro-python3 crcmod dill hdfs seqeval pyjsparser docopt\n",
            "Installing collected packages: pyjsparser, docopt, crcmod, zstandard, tensorflow-model-optimization, tensorflow_io, redis, pyparsing, portalocker, orjson, objsize, js2py, fasteners, fastavro, dnspython, dill, colorama, avro-python3, sacrebleu, pymongo, pydot, hdfs, seqeval, lvis, apache-beam, tensorflow-text, tf-models-official, object_detection\n",
            "  Attempting uninstall: pyparsing\n",
            "    Found existing installation: pyparsing 3.1.4\n",
            "    Uninstalling pyparsing-3.1.4:\n",
            "      Successfully uninstalled pyparsing-3.1.4\n",
            "  Attempting uninstall: pydot\n",
            "    Found existing installation: pydot 3.0.2\n",
            "    Uninstalling pydot-3.0.2:\n",
            "      Successfully uninstalled pydot-3.0.2\n",
            "Successfully installed apache-beam-2.59.0 avro-python3-1.10.2 colorama-0.4.6 crcmod-1.7 dill-0.3.1.1 dnspython-2.6.1 docopt-0.6.2 fastavro-1.9.7 fasteners-0.19 hdfs-2.7.3 js2py-0.74 lvis-0.5.3 object_detection-0.1 objsize-0.7.0 orjson-3.10.7 portalocker-2.10.1 pydot-1.4.2 pyjsparser-2.7.1 pymongo-4.10.1 pyparsing-2.4.7 redis-5.1.1 sacrebleu-2.2.0 seqeval-1.2.2 tensorflow-model-optimization-0.8.0 tensorflow-text-2.17.0 tensorflow_io-0.37.1 tf-models-official-2.17.0 zstandard-0.23.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add libraries to python path\n",
        "import sys\n",
        "sys.path.append('models/research')\n",
        "sys.path.append('models/research/slim')"
      ],
      "metadata": {
        "id": "LkmjYTUOuq_T"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reaccess Dataset Preparation\n",
        "- What we already wrote to the record files"
      ],
      "metadata": {
        "id": "BdJOMh8x3cXc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_record = '/content/training.record'\n",
        "validation_record = '/content/validation.record'"
      ],
      "metadata": {
        "id": "4HH3GqTa2_wR"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configure the Model\n",
        "- Download pretrained model: Coco SSD MobileNet V2 for now\n",
        "- Edit configuration file (.config)\n",
        "  - Paths for training and validation datasets\n",
        "  - Adjust the number of classes / batch size / ect."
      ],
      "metadata": {
        "id": "DlJawl0-3mgS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading the Model from Tensorflow and unpacking zip file\n",
        "!wget https://storage.googleapis.com/download.tensorflow.org/models/tflite/coco_ssd_mobilenet_v1_1.0_quant_2018_06_29.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j9onjyaI4D_F",
        "outputId": "87104e4c-4840-40b3-bf08-e72437298de4"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-10-05 18:17:46--  https://storage.googleapis.com/download.tensorflow.org/models/tflite/coco_ssd_mobilenet_v1_1.0_quant_2018_06_29.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 142.251.183.207, 142.250.125.207, 142.251.184.207, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|142.251.183.207|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2807218 (2.7M) [application/zip]\n",
            "Saving to: ‘coco_ssd_mobilenet_v1_1.0_quant_2018_06_29.zip’\n",
            "\n",
            "\r          coco_ssd_   0%[                    ]       0  --.-KB/s               \rcoco_ssd_mobilenet_ 100%[===================>]   2.68M  --.-KB/s    in 0.02s   \n",
            "\n",
            "2024-10-05 18:17:46 (129 MB/s) - ‘coco_ssd_mobilenet_v1_1.0_quant_2018_06_29.zip’ saved [2807218/2807218]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Unzipping file\n",
        "!unzip /content/coco_ssd_mobilenet_v1_1.0_quant_2018_06_29.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kyadXbRS5v0F",
        "outputId": "39c89a13-7460-4ed2-f340-7e84c57db7f3"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/coco_ssd_mobilenet_v1_1.0_quant_2018_06_29.zip\n",
            "  inflating: detect.tflite           \n",
            "  inflating: labelmap.txt            \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Editing Configuration file\n",
        "!cp coco_ssd_mobilenet_v1_1.0_quant_2018_06_29/pipeline.config /content/pipeline.config"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bxams3tO6iN7",
        "outputId": "83b32e67-6c1f-499a-8104-b3ed4c698ae2"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp: cannot stat 'coco_ssd_mobilenet_v1_1.0_quant_2018_06_29/pipeline.config': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reconfiguring the model\n",
        "https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/training.html"
      ],
      "metadata": {
        "id": "719c9_F5BeBf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow_hub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U8R3iVUrDf51",
        "outputId": "23e53c1c-605b-4b69-e79e-7cc665b497d9"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow_hub in /usr/local/lib/python3.10/dist-packages (0.16.1)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow_hub) (1.26.4)\n",
            "Requirement already satisfied: protobuf>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow_hub) (3.20.3)\n",
            "Requirement already satisfied: tf-keras>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow_hub) (2.17.0)\n",
            "Requirement already satisfied: tensorflow<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tf-keras>=2.14.1->tensorflow_hub) (2.17.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (24.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (71.0.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (2.17.0)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (3.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (0.44.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (13.8.1)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (0.12.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (3.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (2.1.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "# Loading model with tensorflow hub because of http issues...\n",
        "model_url = \"https://tfhub.dev/google/faster_rcnn/openimages_v4/inception_resnet_v2/1\"\n",
        "model = hub.load(model_url)"
      ],
      "metadata": {
        "id": "260g9A_FDkOY"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Tensorflow hub doesn't download the model into the files or directory but instead straight into memory...\n",
        "- I want to see if I can save it as a tangible file so I don't have to load it every time"
      ],
      "metadata": {
        "id": "rZpdm2MnFK3y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!wget -O ssd_resnet50_v1_fpn_640x640.tar.gz \\\n",
        "#http://download.tensorflow.org/models/ssd_resnet50_v1_fpn_640x640.tar.gz\n",
        "# Doesn't like the link...403 error (security???)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gtHmAX8HBc_7",
        "outputId": "ad6a08a1-c8a4-4d21-9f81-9dad42fa1e7a"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-10-05 18:59:23--  http://download.tensorflow.org/models/ssd_resnet50_v1_fpn_640x640.tar.gz\n",
            "Resolving download.tensorflow.org (download.tensorflow.org)... 173.194.206.207, 142.251.183.207, 142.250.125.207, ...\n",
            "Connecting to download.tensorflow.org (download.tensorflow.org)|173.194.206.207|:80... connected.\n",
            "HTTP request sent, awaiting response... 403 Forbidden\n",
            "2024-10-05 18:59:23 ERROR 403: Forbidden.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving model to disk for future use\n",
        "import os\n",
        "\n",
        "# Create directory to save model to\n",
        "save_directory = 'content/faster_rcnn_model'\n",
        "os.makedirs(save_directory, exist_ok=True)\n",
        "\n",
        "# Saving the model\n",
        "tf.saved_model.save(model, save_directory)"
      ],
      "metadata": {
        "id": "4IN_feU0FHUw"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Saved in content/faster_rcnn_model\n",
        "- If want to load model later:\n",
        "\n",
        "\n",
        "```\n",
        "load_model = tf.saved_model.load(save_dir)\n",
        "```\n",
        "(From ChatGPT)\n"
      ],
      "metadata": {
        "id": "zgx397BAF8Vd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reconfiguring once more (Manually downloading the model this time)"
      ],
      "metadata": {
        "id": "Q4f6dyMkNmRO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /content/drive/MyDrive/ && tar -xvf ssd_mobilenet_v2_320x320_coco17_tpu-8.tar.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jv9DPUTaM6Mh",
        "outputId": "f9ba0136-613f-4e1f-f92b-8ce37e3cc81b"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ssd_mobilenet_v2_320x320_coco17_tpu-8/\n",
            "ssd_mobilenet_v2_320x320_coco17_tpu-8/checkpoint/\n",
            "ssd_mobilenet_v2_320x320_coco17_tpu-8/checkpoint/ckpt-0.data-00000-of-00001\n",
            "ssd_mobilenet_v2_320x320_coco17_tpu-8/checkpoint/checkpoint\n",
            "ssd_mobilenet_v2_320x320_coco17_tpu-8/checkpoint/ckpt-0.index\n",
            "ssd_mobilenet_v2_320x320_coco17_tpu-8/pipeline.config\n",
            "ssd_mobilenet_v2_320x320_coco17_tpu-8/saved_model/\n",
            "ssd_mobilenet_v2_320x320_coco17_tpu-8/saved_model/saved_model.pb\n",
            "ssd_mobilenet_v2_320x320_coco17_tpu-8/saved_model/variables/\n",
            "ssd_mobilenet_v2_320x320_coco17_tpu-8/saved_model/variables/variables.data-00000-of-00001\n",
            "ssd_mobilenet_v2_320x320_coco17_tpu-8/saved_model/variables/variables.index\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Configuring the Training Pipeline\n",
        "- Following link instructions still"
      ],
      "metadata": {
        "id": "BvyKohqMNvCh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Copy the pipeline file for this model to the content folder\n",
        "!cd /content/drive/MyDrive/ssd_mobilenet_v2_320x320_coco17_tpu-8 && cp pipeline.config /content/pipeline.config\n",
        "# !cp /pipeline.config /content/pipeline.config"
      ],
      "metadata": {
        "id": "DBuFrNesE-rh"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we need to change the actual file...\n",
        "1. Opening file in Google Colab\n",
        "2. [line 3] num_classes = 1 (We only have \"Lego Friend\")\n",
        "3. [line 138] batch_size = 8 (We can increase/decrease based on memory space later)\n",
        "4. [line 162] fine_tune_checkpoint: \"/content/drive/MyDrive/ssd_mobilenet_v2_320x320_coco17_tpu-8/checkpoint/ckpt-0\"\n",
        "  - DON'T INCLUDE FILE EXTENTIONS\n",
        "  - Tensorflow will go through and look at all files with the ckpt-0 file name later\n",
        "    - More flexibility and abiding by convention\n",
        "    - Explained by ChatGPT\n",
        "5. [line 168] fine_tune_checkpoint_type: \"detection\"\n",
        "  - We want it to be training the whole detection model\n",
        "6. [line 172] label_map_path: \"/content/drive/MyDrive/label_map_legofriend.pbtxt\"\n",
        "7. [line 174] input_path: \"/content/validation.record\"\n",
        "8. [line 182] label_map_path: \"/content/drive/MyDrive/label_map_legofriend.pbtxt\"\n",
        "9. [line 186] input_path: \"/content/validation.record\""
      ],
      "metadata": {
        "id": "KaRKhvqqOW-v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Making a label_map.pbtxt file\n",
        "- We only have one class (Lego Friend), so we only need one item in our label class\n",
        "- This is what our label map file looks like (Created with ChatGPT's help ['cause I didn't understand what the heck this was before I asked it, lol])\n",
        "\n",
        "\n",
        "```\n",
        "item {\n",
        "  id: 1\n",
        "  name: 'Lego Friend'\n",
        "}\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "FFVYXgkyXZ4J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the Model\n",
        "Reference: https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/training.html\n",
        "\n",
        "\n",
        "1. Want to use the /content/models/research/object_detection/model_main_tf2.py script from TensorFlow to train our model\n",
        "  - Copy into our training folder (which for me is the /content file)\n",
        "2. From the directory this file is in (/content), run the command:\n",
        "```\n",
        "python model_main_tf2.py --model_dir=models/my_ssd_resnet50_v1_fpn --pipeline_config_path=models/my_ssd_resnet50_v1_fpn/pipeline.config\n",
        "```\n",
        "  - WAIT for it to run...no touchy!\n",
        "\n",
        "  \n",
        "  ![no touchy](https://meme-generator.com/wp-content/uploads/mememe/2020/03/mememe_d75ef82eed2390ac48ea8a2a472f4a59-1.jpg)\n",
        "\n"
      ],
      "metadata": {
        "id": "Wnyob5VGYF3o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The model_dir here\n",
        "- ALL INFO ABOUT MODEL USED IS IN PIPELINE FILE\n",
        "- The model_dir here means where the training will store the checkpoints\n",
        "- Create new directory for this: /content/drive/MyDrive/model_dir_1"
      ],
      "metadata": {
        "id": "pxloKZu5eF0r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Copying file into workspace\n",
        "!cp /content/models/research/object_detection/model_main_tf2.py /content/drive/MyDrive/model_main_tf2.py"
      ],
      "metadata": {
        "id": "GyJnhv-AZfWo"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Running the long training prompt and filling in values\n",
        "# python model_main_tf2.py --model_dir=models/my_ssd_resnet50_v1_fpn --pipeline_config_path=models/my_ssd_resnet50_v1_fpn/pipeline.config\n",
        "!cd /content/drive/MyDrive/ && python model_main_tf2.py --model_dir=/content/drive/MyDrive/model_dir_1 --pipeline_config_path=pipeline.config --alsologtostderr\n",
        "# Added --alsologtostderr because ChatGPT said this would show logs in the terminal"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_4cxAuGzZxnA",
        "outputId": "e8213abe-f14c-4161-e7b1-5d3c8fc57882"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-10-05 20:58:39.226202: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-10-05 20:58:39.248216: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-10-05 20:58:39.254606: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-10-05 20:58:40.756116: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/model_main_tf2.py\", line 31, in <module>\n",
            "    from object_detection import model_lib_v2\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/object_detection/model_lib_v2.py\", line 30, in <module>\n",
            "    from object_detection import inputs\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/object_detection/inputs.py\", line 24, in <module>\n",
            "    from tensorflow.compat.v1 import estimator as tf_estimator\n",
            "ImportError: cannot import name 'estimator' from 'tensorflow.compat.v1' (/usr/local/lib/python3.10/dist-packages/tensorflow/_api/v2/compat/v1/__init__.py)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exporting Trained Model\n",
        "Instructions from ChatGPT\n",
        "\n",
        "```\n",
        "!python models/research/object_detection/exporter_main_v2.py \\\n",
        "    --input_type=image_tensor \\\n",
        "    --pipeline_config_path=/content/pipeline.config \\\n",
        "    --trained_checkpoint_dir=/content/model_dir \\\n",
        "    --output_directory=/content/exported-model\n",
        "\n",
        "```\n",
        "- According to my understanding from what ChatGPT is saying, I shouldn't have to create any of these directories; training will create them\n",
        "- Training will create checkpoints throughout training, and that's what it's referencing with trained_checkpoint_dir\n"
      ],
      "metadata": {
        "id": "I3gtAkkDaqHl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Exporting, changed pipeline path\n",
        "!python /content/models/research/object_detection/exporter_main_v2.py \\\n",
        "    --input_type=image_tensor \\\n",
        "    --pipeline_config_path=/content/drive/MyDrive/pipeline.config \\\n",
        "    --trained_checkpoint_dir=/content/drive/MyDrive/model_dir_1 \\\n",
        "    --output_directory=/content/drive/MyDrive/exported-model"
      ],
      "metadata": {
        "id": "cnehTBKccJ_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Converting to TensorFlow Lite\n",
        "Instructions from ChatGPT\n",
        "\n",
        "\n",
        "```\n",
        "import tensorflow as tf\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model('/content/exported-model/saved_model')\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "with open('model.tflite', 'wb') as f:\n",
        "    f.write(tflite_model)\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "XBZzObu0bAba"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Downloading the Model\n",
        "Instructions from ChatGPT\n",
        "\n",
        "\n",
        "```\n",
        "from google.colab import files\n",
        "\n",
        "files.download('model.tflite')\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "XWiu3ZY8bIBi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next steps according to ChatGPT\n",
        "\n",
        "    Edit the Configuration File: You need to edit the .config file corresponding to the model you downloaded. You can use sed or any text editor to make the necessary changes. Here's an example:\n",
        "\n",
        "python\n",
        "\n",
        "!cp ssd_mobilenet_v2_coco_2018_03_29/pipeline.config /content/pipeline.config\n",
        "\n",
        "# Edit the pipeline.config file to set paths and hyperparameters\n",
        "!sed -i 's|PATH_TO_BE_CONFIGURED|/content|g' /content/pipeline.config\n",
        "\n",
        "Make sure to set the paths for your training and validation datasets and adjust the number of classes, batch size, etc.\n",
        "Step 5: Train the Model\n",
        "\n",
        "Run the training script using the following command:\n",
        "\n",
        "python\n",
        "\n",
        "!python models/research/object_detection/model_main_tf2.py \\\n",
        "    --pipeline_config_path=/content/pipeline.config \\\n",
        "    --model_dir=/content/model_dir \\\n",
        "    --alsologtostderr \\\n",
        "    --num_steps=5000  # Adjust the number of steps as needed\n",
        "\n",
        "Step 6: Export the Trained Model\n",
        "\n",
        "Once training is complete, you can export the trained model to a TensorFlow SavedModel format or directly to TensorFlow Lite:\n",
        "\n",
        "python\n",
        "\n",
        "!python models/research/object_detection/exporter_main_v2.py \\\n",
        "    --input_type=image_tensor \\\n",
        "    --pipeline_config_path=/content/pipeline.config \\\n",
        "    --trained_checkpoint_dir=/content/model_dir \\\n",
        "    --output_directory=/content/exported-model\n",
        "\n",
        "Step 7: Convert to TensorFlow Lite (Optional)\n",
        "\n",
        "If you want to convert the model to TensorFlow Lite:\n",
        "\n",
        "python\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model('/content/exported-model/saved_model')\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "with open('model.tflite', 'wb') as f:\n",
        "    f.write(tflite_model)\n",
        "\n",
        "Step 8: Download the Model\n",
        "\n",
        "Finally, you can download your model or any files created during the process:\n",
        "\n",
        "python\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "files.download('model.tflite')"
      ],
      "metadata": {
        "id": "LiKAcRwJHZtN"
      }
    }
  ]
}